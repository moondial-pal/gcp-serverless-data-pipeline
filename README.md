# GCP Serverless Data Pipeline

This project implements a serverless data pipeline on Google Cloud Platform (GCP) using Terraform-based Infrastructure as Code. The system ingests CSV datasets uploaded to Cloud Storage, triggers event-driven processing, applies lightweight transformations, and loads structured results into BigQuery for downstream analytics.

Designed as a portfolio project, it emphasizes practical cloud architecture concerns including cost efficiency, least-privilege IAM, scalability, and operational simplicity.

---

## ğŸ“¦ Features

- â˜ï¸ **Serverless architecture**
  - Cloud Storage triggers serverless compute to process CSV uploads
  - Transformed data is stored in BigQuery for analysis
- ğŸ”’ **Security by design**
  - Least privilege IAM roles
  - Service account isolation
  - Secure GCS buckets
- ğŸ—ï¸ **Infrastructure as Code**
  - Modular Terraform configuration
  - Remote backend with GCS for state management

## ğŸ“¦ Planned Enhancements
These enhancements are intentionally deferred to keep the initial design focused on core data flow and infrastructure boundaries.

- ğŸš€ **Deployment automation**
  - Add Cloud Build triggers for automated infrastructure and service deployments
- ğŸ“Š **Observability**
  - Integrate Cloud Logging and Cloud Monitoring dashboards
  - Add uptime checks for critical endpoints

---

## ğŸ› ï¸ Technologies

- **Google Cloud Platform (GCP)**
  - Cloud Storage
  - Cloud Run Functions (Python)
  - BigQuery
  - IAM
- **Terraform** (modular setup with GCS remote backend)
- **Python** for serverless data transformation

---

## ğŸ“ Architecture

[Client Uploads CSV] â†’ [Cloud Storage Bucket] â†’ [Cloud Function (ETL)]
â†“
[Processed Data in BigQuery] â†’ [Downstream Analytics / BI Tools]


 Cloud Storage bucket acts as an ingestion point for CSV uploads.
- Cloud Run Function is triggered on object creation:
  - Validates and transforms data.
  - Loads processed data into BigQuery.
- The pipeline is designed to be stateless and event-driven, with IAM-scoped service accounts and no long-lived credentials.

---

## ğŸš€ Getting Started

### Prerequisites
- [Terraform](https://developer.hashicorp.com/terraform/downloads) v1.5+
- [gcloud CLI](https://cloud.google.com/sdk/docs/install)
- Python 3.10+
- GCP project with billing enabled

### Clone Repository
```bash
git clone https://github.com/moondial-pal/gcp-serverless-data-pipeline.git
cd gcp-serverless-data-pipeline
```
Initialize Terraform
```Bash
terraform init
```
Deploy Infrastructure
```Bash
terraform apply
```
Deploy Cloud Function
```Bash
gcloud functions deploy process_csv \
  --runtime python310 \
  --trigger-bucket <your-ingestion-bucket> \
  --entry-point main \
  --source cloud_run_function/
```

## ğŸ“„ Documentation

  [Architecture Overview](https://www.gcp-serverless-data-pipeline-architecture/luispal.com)

  [Terraform Modules](https://www.gcp-serverless-data-pipeline-tfmods/luispal.com)

  [Runbook](https://www.gcp-serverless-data-pipeline-runbook/luispal.com)
